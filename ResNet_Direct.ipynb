{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cd7134-67aa-4aab-b287-26df611b5755",
   "metadata": {
    "id": "36cd7134-67aa-4aab-b287-26df611b5755"
   },
   "source": [
    "`COLAB` determines whether this notebook is running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470321f4-c578-4ca8-a7eb-7eb89932ffa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "470321f4-c578-4ca8-a7eb-7eb89932ffa9",
    "outputId": "0067d1a4-16da-456d-976f-10c3158fcfb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLAB=False\n"
     ]
    }
   ],
   "source": [
    "COLAB = 'google.colab' in str(get_ipython())\n",
    "print(f'COLAB={COLAB}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9a41e1-ae51-4dda-83d2-ba725991c793",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f9a41e1-ae51-4dda-83d2-ba725991c793",
    "outputId": "e3872a16-e9c9-4020-a938-a1865aa9e9a1"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # clone github repo\n",
    "    !git clone --depth 1 https://github.com/Klinjin/WL_ML.git\n",
    "    # move to the HEP starting kit folder\n",
    "    %cd WL_ML/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0f37b-38ce-4471-bd28-175323314b9b",
   "metadata": {
    "id": "30e0f37b-38ce-4471-bd28-175323314b9b"
   },
   "source": [
    "# 0 - Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a956880-e2b9-4868-8446-ad66302d9211",
   "metadata": {
    "id": "0a956880-e2b9-4868-8446-ad66302d9211"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c0b8a",
   "metadata": {
    "id": "509c0b8a"
   },
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1237f004",
   "metadata": {
    "id": "1237f004"
   },
   "outputs": [],
   "source": [
    "class Utility:\n",
    "    @staticmethod\n",
    "    def add_noise(data, mask, ng, pixel_size=2.):\n",
    "        \"\"\"\n",
    "        Add noise to a noiseless convergence map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array\n",
    "            Noiseless convergence maps.\n",
    "        mask : np.array\n",
    "            Binary mask map.\n",
    "        ng : float\n",
    "            Number of galaxies per arcmin². This determines the noise level; a larger number means smaller noise.\n",
    "        pixel_size : float, optional\n",
    "            Pixel size in arcminutes (default is 2.0).\n",
    "        \"\"\"\n",
    "\n",
    "        return data + np.random.randn(*data.shape) * 0.4 / (2*ng*pixel_size**2)**0.5 * mask\n",
    "\n",
    "    @staticmethod\n",
    "    def load_np(data_dir, file_name):\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        return np.load(file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_np(data_dir, file_name, data):\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        np.save(file_path, data)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_json_zip(submission_dir, json_file_name, zip_file_name, data):\n",
    "        \"\"\"\n",
    "        Save a dictionary with 'means' and 'errorbars' into a JSON file,\n",
    "        then compress it into a ZIP file inside submission_dir.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        submission_dir : str\n",
    "            Path to the directory where the ZIP file will be saved.\n",
    "        file_name : str\n",
    "            Name of the ZIP file (without extension).\n",
    "        data : dict\n",
    "            Dictionary with keys 'means' and 'errorbars'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Path to the created ZIP file.\n",
    "        \"\"\"\n",
    "        os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "        json_path = os.path.join(submission_dir, json_file_name)\n",
    "\n",
    "        # Save JSON file\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "        # Path to ZIP\n",
    "        zip_path = os.path.join(submission_dir, zip_file_name)\n",
    "\n",
    "        # Create ZIP containing only the JSON\n",
    "        with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "            zf.write(json_path, arcname=json_file_name)\n",
    "\n",
    "        # Remove the standalone JSON after zipping\n",
    "        os.remove(json_path)\n",
    "\n",
    "        return zip_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebf1d0",
   "metadata": {
    "id": "d7ebf1d0"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e3a3d1",
   "metadata": {
    "id": "e2e3a3d1"
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, data_dir, USE_PUBLIC_DATASET):\n",
    "        self.USE_PUBLIC_DATASET = USE_PUBLIC_DATASET\n",
    "        self.data_dir = data_dir\n",
    "        self.mask_file = 'WIDE12H_bin2_2arcmin_mask.npy'\n",
    "        self.viz_label_file = 'label.npy'\n",
    "\n",
    "        if self.USE_PUBLIC_DATASET:\n",
    "            self.kappa_file = 'WIDE12H_bin2_2arcmin_kappa.npy'\n",
    "            self.label_file = self.viz_label_file\n",
    "            self.Ncosmo = 101  # Number of cosmologies in the entire training data\n",
    "            self.Nsys = 256    # Number of systematic realizations in the entire training data\n",
    "            self.test_kappa_file = 'WIDE12H_bin2_2arcmin_kappa_noisy_test.npy'\n",
    "            self.Ntest = 4000  # Number of instances in the test data\n",
    "        else:\n",
    "            self.kappa_file = 'sampled_WIDE12H_bin2_2arcmin_kappa.npy'\n",
    "            self.label_file = 'sampled_label.npy'\n",
    "            self.Ncosmo = 3    # Number of cosmologies in the sampled training data\n",
    "            self.Nsys = 30     # Number of systematic realizations in the sampled training data\n",
    "            self.test_kappa_file = 'sampled_WIDE12H_bin2_2arcmin_kappa_noisy_test.npy'\n",
    "            self.Ntest = 3     # Number of instances in the sampled test data\n",
    "\n",
    "        self.shape = [1424,176] # dimensions of each map\n",
    "        self.pixelsize_arcmin = 2 # pixel size in arcmin\n",
    "        self.pixelsize_radian = self.pixelsize_arcmin / 60 / 180 * np.pi # pixel size in radian\n",
    "        self.ng = 30  # galaxy number density. This determines the noise level of the experiment. Do not change this number.\n",
    "        self.mask = Utility.load_np(data_dir=self.data_dir, file_name=self.mask_file) # A binary map that shows which parts of the sky are observed and which areas are blocked\n",
    "        self.label = Utility.load_np(data_dir=self.data_dir, file_name=self.label_file) # Training labels (cosmological and physical paramameters) of each training map\n",
    "        self.viz_label = Utility.load_np(data_dir=self.data_dir, file_name=self.viz_label_file) # For visualization of parameter distributions\n",
    "\n",
    "    def load_train_data(self):\n",
    "        self.kappa = np.zeros((self.Ncosmo, self.Nsys, *self.shape), dtype=np.float16)\n",
    "        self.kappa[:,:,self.mask] = Utility.load_np(data_dir=self.data_dir, file_name=self.kappa_file) # Training convergence maps\n",
    "\n",
    "    def load_test_data(self):\n",
    "        self.kappa_test = np.zeros((self.Ntest, *self.shape), dtype=np.float16)\n",
    "        self.kappa_test[:,self.mask] = Utility.load_np(data_dir=self.data_dir, file_name=self.test_kappa_file) # Test noisy convergence maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6fa112",
   "metadata": {
    "id": "0a6fa112"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1245b15e",
   "metadata": {
    "id": "1245b15e"
   },
   "outputs": [],
   "source": [
    "class Visualization:\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_mask(mask):\n",
    "        plt.figure(figsize=(30,100))\n",
    "        plt.imshow(mask.T)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_noiseless_training_convergence_map(kappa):\n",
    "        plt.figure(figsize=(30,100))\n",
    "        plt.imshow(kappa[0,0].T, vmin=-0.02, vmax=0.07)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_noisy_training_convergence_map(kappa, mask, pixelsize_arcmin, ng):\n",
    "        plt.figure(figsize=(30,100))\n",
    "        plt.imshow(Utility.add_noise(kappa[0,0], mask, ng, pixelsize_arcmin).T, vmin=-0.02, vmax=0.07)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_cosmological_parameters_OmegaM_S8(label):\n",
    "        plt.scatter(label[:,0,0], label[:,0,1])\n",
    "        plt.xlabel(r'$\\Omega_m$')\n",
    "        plt.ylabel(r'$S_8$')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_baryonic_physics_parameters(label):\n",
    "        plt.scatter(label[0,:,2], label[0,:,3])\n",
    "        plt.xlabel(r'$T_{\\mathrm{AGN}}$')\n",
    "        plt.ylabel(r'$f_0$')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_photometric_redshift_uncertainty_parameters(label):\n",
    "        plt.hist(label[0,:,4], bins=20)\n",
    "        plt.xlabel(r'$\\Delta z$')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a6c33",
   "metadata": {
    "id": "ba4a6c33"
   },
   "source": [
    "### Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef5773e",
   "metadata": {
    "id": "9ef5773e"
   },
   "outputs": [],
   "source": [
    "class Score:\n",
    "    @staticmethod\n",
    "    def _score_phase1(true_cosmo, infer_cosmo, errorbar):\n",
    "        \"\"\"\n",
    "        Computes the log-likelihood score for Phase 1 based on predicted cosmological parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        true_cosmo : np.ndarray\n",
    "            Array of true cosmological parameters (shape: [n_samples, n_params]).\n",
    "        infer_cosmo : np.ndarray\n",
    "            Array of inferred cosmological parameters from the model (same shape as true_cosmo).\n",
    "        errorbar : np.ndarray\n",
    "            Array of standard deviations (uncertainties) for each inferred parameter\n",
    "            (same shape as true_cosmo).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of scores for each sample (shape: [n_samples]).\n",
    "        \"\"\"\n",
    "\n",
    "        sq_error = (true_cosmo - infer_cosmo)**2\n",
    "        scale_factor = 1000  # This is a constant that scales the error term.\n",
    "        score = - np.sum(sq_error / errorbar**2 + np.log(errorbar**2) + scale_factor * sq_error, 1)\n",
    "        score = np.mean(score)\n",
    "        if score >= -10**6: # Set a minimum of the score (to properly display on Codabench)\n",
    "            return score\n",
    "        else:\n",
    "            return -10**6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542aaf9b-5e06-42c3-97af-d82a9a1a6bbc",
   "metadata": {
    "id": "542aaf9b-5e06-42c3-97af-d82a9a1a6bbc"
   },
   "source": [
    "# 2 - Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a56be65b-19a7-4353-a752-c07d2512c24e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a56be65b-19a7-4353-a752-c07d2512c24e",
    "outputId": "6437d315-4417-4df3-e9e2-6f9dbfc97cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is /pscratch/sd/l/lindajin/WL_ML\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069869cd-9501-47a5-8c32-c8f3a7f25756",
   "metadata": {
    "id": "069869cd-9501-47a5-8c32-c8f3a7f25756"
   },
   "outputs": [],
   "source": [
    "USE_PUBLIC_DATASET = True\n",
    "\n",
    "# USE_PUBLIC_DATASET = True\n",
    "PUBLIC_DATA_DIR = os.path.join(root_dir, 'input_data/')  # This is only required when you set USE_PUBLIC_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be959a29-b52e-4e8e-b20e-e6f2c507b710",
   "metadata": {
    "id": "be959a29-b52e-4e8e-b20e-e6f2c507b710"
   },
   "outputs": [],
   "source": [
    "if not USE_PUBLIC_DATASET:                                         # Testing this startking kit with a tiny sample of the training data (3, 30, 1424, 176)\n",
    "    DATA_DIR = os.path.join(root_dir, 'input_data/')\n",
    "else:                                                              # Training your model with all training data (101, 256, 1424, 176)\n",
    "    DATA_DIR = PUBLIC_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51693e-73ba-4f29-adf0-7a52785f210e",
   "metadata": {
    "id": "ff51693e-73ba-4f29-adf0-7a52785f210e"
   },
   "source": [
    "### Load the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4be679",
   "metadata": {
    "id": "fc4be679"
   },
   "outputs": [],
   "source": [
    "# Initialize Data class object\n",
    "data_obj = Data(data_dir=DATA_DIR, USE_PUBLIC_DATASET=USE_PUBLIC_DATASET)\n",
    "\n",
    "# # Load train data\n",
    "# data_obj.load_train_data()\n",
    "\n",
    "# Load test data\n",
    "data_obj.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b475663b-ca7c-4443-af4a-e1e3eb416a1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b475663b-ca7c-4443-af4a-e1e3eb416a1e",
    "outputId": "9bb1aa3b-1b56-4b6c-aa00-defd3bb2a655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 101 cosmological models, each has 256 realizations of nuisance parameters in the training data.\n",
      "We assume a galaxy number density of 30 per arcmin², which determines the noise level of the experiment.\n"
     ]
    }
   ],
   "source": [
    "Ncosmo = data_obj.Ncosmo\n",
    "Nsys = data_obj.Nsys\n",
    "ng = data_obj.ng\n",
    "\n",
    "print(f'There are {Ncosmo} cosmological models, each has {Nsys} realizations of nuisance parameters in the training data.')\n",
    "print(f'We assume a galaxy number density of {ng} per arcmin², which determines the noise level of the experiment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1b8d763-8a2e-477f-8b83-061e2717004d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1b8d763-8a2e-477f-8b83-061e2717004d",
    "outputId": "05dfa1eb-6b97-40bb-ab43-a6bff6597d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data = (101, 256, 1424, 176)\n",
      "Shape of the mask = (1424, 176)\n",
      "Shape of the training label = (101, 256, 5)\n",
      "Shape of the test data = (4000, 1424, 176)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the training data = {data_obj.kappa.shape}')\n",
    "print(f'Shape of the mask = {data_obj.mask.shape}')\n",
    "print(f'Shape of the training label = {data_obj.label.shape}')\n",
    "print(f'Shape of the test data = {data_obj.kappa_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ff43f8e-f8e8-485d-ba56-24eb82293ce6",
   "metadata": {
    "id": "5ff43f8e-f8e8-485d-ba56-24eb82293ce6"
   },
   "outputs": [],
   "source": [
    "# Add the pixel-level noise to the training set (note that this may take some time and large memory) ~140 GB\n",
    "\n",
    "np.random.seed(31415)  # Fix the random seed for reproducible results\n",
    "noisy_kappa = Utility.add_noise(data=data_obj.kappa.astype(np.float64),\n",
    "                                mask=data_obj.mask,\n",
    "                                ng=data_obj.ng,\n",
    "                                pixel_size=data_obj.pixelsize_arcmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1e0a352-8abd-41f2-a595-632fdb7c1c4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1e0a352-8abd-41f2-a595-632fdb7c1c4d",
    "outputId": "b5451ad7-e6f1-4a38-a133-d5af37e59d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the noised data (101, 256, 1424, 176)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the noised data {noisy_kappa.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e88dd-7653-4188-bbab-a23ebd6264e0",
   "metadata": {
    "id": "eb0e88dd-7653-4188-bbab-a23ebd6264e0"
   },
   "source": [
    "#### ⚠️ NOTE:\n",
    "\n",
    "If you want to split your own training/validation sets to evaluate your model, we recommend splitting the original training data along `axis = 1` (the 256 realizations of nuisance parameters). This will ensure that there are no intrinsic correlations between the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3ac5ba29-70db-48c5-8c4f-98709ba6779b",
   "metadata": {
    "id": "3ac5ba29-70db-48c5-8c4f-98709ba6779b"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "\n",
    "NP_idx = np.arange(Nsys)  # The indices of Nsys nuisance parameter realizations\n",
    "split_fraction = 0.2      # Set the fraction of data you want to split (between 0 and 1)\n",
    "seed = 5566               # Define your random seed for reproducible results\n",
    "\n",
    "train_NP_idx, val_NP_idx = train_test_split(NP_idx, test_size=split_fraction,\n",
    "                                            random_state=seed)\n",
    "\n",
    "noisy_kappa_train = noisy_kappa[:, train_NP_idx]      # shape = (Ncosmo, len(train_NP_idx), 1424, 176)\n",
    "label_train = data_obj.label[:, train_NP_idx]         # shape = (Ncosmo, len(train_NP_idx), 5)\n",
    "noisy_kappa_val = noisy_kappa[:, val_NP_idx]          # shape = (Ncosmo, len(val_NP_idx), 1424, 176)\n",
    "label_val = data_obj.label[:, val_NP_idx]             # shape = (Ncosmo, len(val_NP_idx), 5)\n",
    "\n",
    "Ntrain = label_train.shape[0]*label_train.shape[1]\n",
    "Nval = label_val.shape[0]*label_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "557d127d-000e-472c-9b88-2ca1e182ed48",
   "metadata": {
    "id": "557d127d-000e-472c-9b88-2ca1e182ed48"
   },
   "outputs": [],
   "source": [
    "# print(f'Shape of the split training data = {noisy_kappa_train.shape}')\n",
    "# print(f'Shape of the split validation data = {noisy_kappa_val.shape}')\n",
    "\n",
    "# print(f'Shape of the split training labels = {label_train.shape}')\n",
    "# print(f'Shape of the split validation labels = {label_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72c5214-1213-46ae-9431-f71433f27b42",
   "metadata": {
    "id": "b72c5214-1213-46ae-9431-f71433f27b42"
   },
   "outputs": [],
   "source": [
    "# # Save the split data and labels for future usage\n",
    "\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_train.npy\",data=noisy_kappa_train)\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"label_train.npy\",data=label_train)\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_val.npy\",data=noisy_kappa_val)\n",
    "# Utility.save_np(data_dir=DATA_DIR, file_name=\"label_val.npy\",data=label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff8bb2f-0acf-48bc-90d5-58e0c1e38709",
   "metadata": {
    "id": "6ff8bb2f-0acf-48bc-90d5-58e0c1e38709"
   },
   "outputs": [],
   "source": [
    "# Load the saved split data (if you saved it at DATA_DIR before)\n",
    "\n",
    "noisy_kappa_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_train.npy\")\n",
    "label_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_train.npy\")\n",
    "noisy_kappa_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_val.npy\")\n",
    "label_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_val.npy\")\n",
    "\n",
    "Ntrain = label_train.shape[0]*label_train.shape[1]\n",
    "Nval = label_val.shape[0]*label_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b77f1c-1c2b-46ef-9247-147f914fc968",
   "metadata": {
    "id": "d1b77f1c-1c2b-46ef-9247-147f914fc968"
   },
   "outputs": [],
   "source": [
    "# Reshape the data for CNN\n",
    "X_train = noisy_kappa_train.reshape(Ntrain, *data_obj.shape)\n",
    "X_val = noisy_kappa_val.reshape(Nval, *data_obj.shape)\n",
    "\n",
    "# Here, we ignore the nuisance parameters and only keep the 2 cosmological parameters\n",
    "y_train = label_train.reshape(Ntrain, 5)[:, :2]\n",
    "y_val = label_val.reshape(Nval, 5)[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04ad216b-9db5-4ea4-ac26-48004b611e0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04ad216b-9db5-4ea4-ac26-48004b611e0d",
    "outputId": "0e0a6870-0d98-4665-9a04-9df320e95acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the split training data = (20604, 1424, 176)\n",
      "Shape of the split validation data = (5252, 1424, 176)\n",
      "Shape of the split training labels = (20604, 2)\n",
      "Shape of the split validation labels = (5252, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the split training data = {X_train.shape}')\n",
    "print(f'Shape of the split validation data = {X_val.shape}')\n",
    "\n",
    "print(f'Shape of the split training labels = {y_train.shape}')\n",
    "print(f'Shape of the split validation labels = {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efab9b-9e44-4ab2-bc89-ac81adcd99a4",
   "metadata": {
    "id": "41efab9b-9e44-4ab2-bc89-ac81adcd99a4"
   },
   "source": [
    "# 4 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31455c2-da5f-4cfa-bfd8-3fc4192bece5",
   "metadata": {
    "id": "d31455c2-da5f-4cfa-bfd8-3fc4192bece5"
   },
   "source": [
    "In this notebook, we do not fit any predefined summary statistics to the data and perform MCMC. Instead, we estimate the uncertainties directly using the CNN. This is achieved by optimizing a KL divergence objective function using neural network predictions during training. For each 2D map, the CNN will predict its cosmological parameters $(\\hat{\\Omega}_m, \\hat{S}_8)$ and the standard deviations of the joint Gaussian posterior distribution $(\\hat{\\sigma}_{\\Omega_m}, \\hat{\\sigma}_{S_8})$.\n",
    "\n",
    "The loss funciton here is a KL divergence objective function defined by\n",
    "$$\n",
    "\\text{KL Loss}= \\frac{1}{N} \\sum_i^{N}\\left\\{\\frac{\\left(\\hat{\\Omega}_{m, i}-\\Omega_{m, i}^{\\text {truth }}\\right)^2}{\\hat{\\sigma}_{\\Omega_m, i}^2}+\\frac{\\left(\\hat{S}_{8, i}-S_{8, i}^{\\text {truth }}\\right)^2}{\\hat{\\sigma}_{S_8, i}^2}+\\log \\left(\\hat{\\sigma}_{\\Omega_m, i}^2\\right)+\\log \\left(\\hat{\\sigma}_{S_8, i}^2\\right)\\right\\}~.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa451c18-7541-400a-9d68-9308d99bd89c",
   "metadata": {
    "id": "fa451c18-7541-400a-9d68-9308d99bd89c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define your path for saving the trained model\n",
    "MODEL_NAME = \"ResNetWithAttention_direct_baseline\"\n",
    "\n",
    "class Config:\n",
    "    IMG_HEIGHT = data_obj.shape[0]\n",
    "    IMG_WIDTH = data_obj.shape[1]\n",
    "\n",
    "    # Parameters to predict (Omega_m, S_8)\n",
    "    NUM_TARGETS = 4\n",
    "\n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 15\n",
    "    LEARNING_RATE = 2e-4\n",
    "    WEIGHT_DECAY = 1e-4   # L2 regularization to prevent overfitting\n",
    "\n",
    "    # DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    MODEL_SAVE_PATH = os.path.join(root_dir, f\"trained_model/{MODEL_NAME}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6252cb9-4a28-4aa7-83ba-ef2bbbfc570a",
   "metadata": {
    "id": "b6252cb9-4a28-4aa7-83ba-ef2bbbfc570a"
   },
   "outputs": [],
   "source": [
    "def KL_div_posterior_loss(pred_means, pred_sigmas, truths):\n",
    "    \"\"\"\n",
    "    A KL divergence loss function that directly optimizes the score function\n",
    "\n",
    "    Inputs:\n",
    "    - pred_means:   2D tensor (batch_size, 2)\n",
    "    - pred_sigmas:  2D tensor (batch_size, 2)\n",
    "    - truths:       2D tensor (batch_size, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    residuals_sq = (pred_means - truths)**2\n",
    "\n",
    "    loss_terms = residuals_sq / (pred_sigmas**2)\n",
    "    loss_sum = torch.sum(loss_terms, dim=1)\n",
    "\n",
    "    log_sigma_terms = torch.sum(torch.log(pred_sigmas**2), dim=1)\n",
    "    loss = torch.mean(loss_sum + log_sigma_terms)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dda60f5-b075-4492-884d-094ad60ced74",
   "metadata": {
    "id": "6dda60f5-b075-4492-884d-094ad60ced74"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Training\")\n",
    "    for X, y in pbar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred_means, pred_sigmas= model(X)\n",
    "        loss = loss_fn(pred_means, pred_sigmas, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    \"\"\"Validates the model on the validation/test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Validating\")\n",
    "    with torch.no_grad():\n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_means, pred_sigmas = model(X)\n",
    "            total_loss += loss_fn(pred_means, pred_sigmas, y).item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0171198-8a56-47a4-9d1c-bf754257d7b8",
   "metadata": {
    "id": "d0171198-8a56-47a4-9d1c-bf754257d7b8"
   },
   "outputs": [],
   "source": [
    "class CosmologyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, labels=None,\n",
    "                 transform=None,\n",
    "                 label_transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert NumPy array to float64 before transform\n",
    "        image = self.data[idx].astype(np.float64)   \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # Convert PyTorch tensor to double precision\n",
    "            image = image.double()\n",
    "        else:\n",
    "            # If no transform, convert NumPy to PyTorch tensor manually\n",
    "            image = torch.from_numpy(image).double()\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx].astype(np.float64)\n",
    "            label = torch.from_numpy(label).double()  # Ensure double precision\n",
    "            if self.label_transform:\n",
    "                label = self.label_transform(label)\n",
    "                label = label.double()  # Ensure still double after transform\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a80b5a6-9c1b-4be4-abed-0b061055d671",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a80b5a6-9c1b-4be4-abed-0b061055d671",
    "outputId": "b26fe731-0ea9-4d9e-ed83-c1165ec0d98a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image stats (from train set): Mean=-0.00014721968909725547, Std=0.020474432036280632\n",
      "Label stats (from train set): Mean=[0.29021683 0.81345297], Std=[0.1055216  0.06600116]\n"
     ]
    }
   ],
   "source": [
    "# Image standardization\n",
    "from torchvision import transforms\n",
    "# Compute the means and stds of the training images (for standardizing the data)\n",
    "\n",
    "means = np.mean(X_train, dtype=np.float32)\n",
    "stds = np.std(X_train, dtype=np.float32)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[means], std=[stds]),\n",
    "])\n",
    "print(f\"Image stats (from train set): Mean={means}, Std={stds}\")\n",
    "\n",
    "# Label standardization\n",
    "label_scaler = StandardScaler()\n",
    "y_train_scaled = label_scaler.fit_transform(y_train)\n",
    "y_val_scaled = label_scaler.transform(y_val)\n",
    "print(f\"Label stats (from train set): Mean={label_scaler.mean_}, Std={np.sqrt(label_scaler.var_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66f83eef-6a50-4560-a5c0-1036bce7a650",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66f83eef-6a50-4560-a5c0-1036bce7a650",
    "outputId": "07795851-a92e-4d93-d758-da65d1f0e172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration\n",
    "config = Config()\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = CosmologyDataset(\n",
    "    data=X_train,\n",
    "    labels=y_train_scaled,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset = CosmologyDataset(\n",
    "    data=X_val,\n",
    "    labels=y_val_scaled,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe1d1060-d2fc-4579-87ea-e81eb21926f5",
   "metadata": {
    "id": "fe1d1060-d2fc-4579-87ea-e81eb21926f5"
   },
   "outputs": [],
   "source": [
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import BigGANUNet2DModel, ResNetWithAttention\n",
    "\n",
    "\n",
    "model = ResNetWithAttention(config.IMG_HEIGHT,\n",
    "                    config.IMG_WIDTH,\n",
    "                    config.NUM_TARGETS).to(config.DEVICE)\n",
    "\n",
    "# model = BigGANUNet2DModel(config.IMG_HEIGHT,\n",
    "#                     config.IMG_WIDTH,\n",
    "#                     config.NUM_TARGETS,\n",
    "#                     n_channels=1,\n",
    "#                     ch_mult = (1, 2, 1),\n",
    "#                          attention=True).to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae4dee-515d-4404-b8af-1601ffa0f79c",
   "metadata": {
    "id": "e2ae4dee-515d-4404-b8af-1601ffa0f79c"
   },
   "source": [
    "Set `USE_PRETRAINED_MODEL = False` if you want to train a new model.\\\n",
    "Set `USE_PRETRAINED_MODEL = True` if you want to load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb4e7b94-3579-49ef-b13a-fc60bbb079ff",
   "metadata": {
    "id": "cb4e7b94-3579-49ef-b13a-fc60bbb079ff"
   },
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL = False\n",
    "# USE_PRETRAINED_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b93c37-9e07-4260-a1da-938a63ecca18",
   "metadata": {
    "id": "05b93c37-9e07-4260-a1da-938a63ecca18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 32/322 [04:58<40:14,  8.32s/it] "
     ]
    }
   ],
   "source": [
    "if not USE_PRETRAINED_MODEL:\n",
    "    # Train the model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                lr=config.LEARNING_RATE,\n",
    "                                weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(optimizer,\n",
    "                                  mode='min',\n",
    "                                  factor=0.5,\n",
    "                                  patience=5)\n",
    "    # Training Loop\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    early_stopping_patience = config.EPOCHS // 4\n",
    "    start_time = time.time()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, KL_div_posterior_loss, optimizer, config.DEVICE)\n",
    "        val_loss = validate_epoch(model, val_loader, KL_div_posterior_loss, config.DEVICE)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Save the best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), config.MODEL_SAVE_PATH)\n",
    "            print(f\"  -> New best model saved to {config.MODEL_SAVE_PATH}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping to prevent overfitting\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs (no improvement for {early_stopping_patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "\n",
    "    # Plot learning curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    model.load_state_dict(torch.load(config.MODEL_SAVE_PATH, weights_only=True)) # Directly load the best model\n",
    "\n",
    "else:\n",
    "    # Check if the pretrained model exists\n",
    "    if os.path.exists(config.MODEL_SAVE_PATH):\n",
    "        # If the pretrained model exists, load the model\n",
    "        model.load_state_dict(torch.load(config.MODEL_SAVE_PATH, weights_only=True))\n",
    "\n",
    "    else:\n",
    "        # If the pretrained model doesn't exist, show the warning message\n",
    "        warning_msg = f\"The path of pretrained model doesn't exist\"\n",
    "        warnings.warn(warning_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda16158",
   "metadata": {
    "id": "eda16158",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2857b3",
   "metadata": {
    "id": "af2857b3"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "  class Config:\n",
    "    IMG_HEIGHT = data_obj.shape[0]\n",
    "    IMG_WIDTH = data_obj.shape[1]\n",
    "\n",
    "    # Parameters to predict (Omega_m, S_8, sigma_Omega_m, sigma_S_8)\n",
    "    NUM_TARGETS = 4\n",
    "\n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE =  trial.suggest_categorical(\"BATCH_SIZE\", [4,8,16,32])\n",
    "    EPOCHS = trial.suggest_int(\"EPOCHS\", 15, 200, step=50)\n",
    "    LEARNING_RATE = trial.suggest_float(\"LEARNING_RATE\", 2e-4, 5e-3, log=True)\n",
    "    WEIGHT_DECAY = 1e-4   # L2 regularization to prevent overfitting\n",
    "\n",
    "    DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    MODEL_SAVE_PATH = os.path.join(root_dir, f\"{MODEL_NAME}.pth\")\n",
    "  config = Config()\n",
    "\n",
    "\n",
    "  train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "  model = Simple_CNN(config.IMG_HEIGHT,\n",
    "                    config.IMG_WIDTH,\n",
    "                    config.NUM_TARGETS).to(config.DEVICE)\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=config.LEARNING_RATE,\n",
    "                                weight_decay=config.WEIGHT_DECAY)\n",
    "  scheduler = ReduceLROnPlateau(optimizer,\n",
    "                                  mode='min',\n",
    "                                  factor=0.5,\n",
    "                                  patience=5)\n",
    "    # Training Loop\n",
    "  best_val_loss = float('inf')\n",
    "  for epoch in range(config.EPOCHS):\n",
    "      train_loss = train_epoch(model, train_loader, KL_div_posterior_loss, optimizer, config.DEVICE)\n",
    "      val_loss = validate_epoch(model, val_loader, KL_div_posterior_loss, config.DEVICE)\n",
    "\n",
    "      scheduler.step(val_loss)\n",
    "      print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Save the best model based on validation loss\n",
    "      if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "\n",
    "  return best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294230d-cb68-41a2-a94a-c65d9d877012",
   "metadata": {
    "id": "b294230d-cb68-41a2-a94a-c65d9d877012"
   },
   "source": [
    "# 5 - Inference on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702162ba-e9e8-41f0-8a8f-5c48d0969ed2",
   "metadata": {
    "id": "702162ba-e9e8-41f0-8a8f-5c48d0969ed2"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "means_pred_list, sigmas_pred_list = [], []\n",
    "pbar = tqdm(val_loader, total=len(val_loader), desc=\"Validating\")\n",
    "with torch.no_grad():\n",
    "    for X, _ in pbar:\n",
    "        X = X.to(config.DEVICE)\n",
    "        means_pred, sigmas_pred = model(X)\n",
    "        means_pred_list.append(means_pred.cpu().numpy())\n",
    "        sigmas_pred_list.append(sigmas_pred.cpu().numpy())\n",
    "\n",
    "mean_val = np.concatenate(means_pred_list, axis=0)\n",
    "mean_val = label_scaler.inverse_transform(mean_val)          # inverse transform\n",
    "\n",
    "errorbar_val = np.concatenate(sigmas_pred_list, axis=0)\n",
    "errorbar_val = errorbar_val*label_scaler.var_**0.5           # rescale by the training label std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b95b7c-fa69-4732-aa0e-84a40234f78c",
   "metadata": {
    "id": "48b95b7c-fa69-4732-aa0e-84a40234f78c"
   },
   "outputs": [],
   "source": [
    "## Include the prior that the cosmological parameters are not negative\n",
    "negative_mask = mean_val - errorbar_val < 0\n",
    "errorbar_val[negative_mask] = mean_val[negative_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6cd49-508d-4f27-b64f-4c91ad5b7507",
   "metadata": {
    "id": "7bb6cd49-508d-4f27-b64f-4c91ad5b7507"
   },
   "outputs": [],
   "source": [
    "# Comparison of the means & standard deviations of the posterior distributions and the validation labels\n",
    "\n",
    "plt.errorbar(y_val[:,0], mean_val[:,0], yerr=errorbar_val[:,0],\n",
    "             fmt='o', capsize=3, capthick=1, ecolor='grey')\n",
    "plt.plot(sorted(y_val[:,0]), sorted(y_val[:,0]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,0]), np.max(y_val[:,0]))\n",
    "plt.ylim(0, 0.7)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$\\Omega_m$')\n",
    "plt.show()\n",
    "\n",
    "plt.errorbar(y_val[:,1], mean_val[:,1], yerr=errorbar_val[:,1],\n",
    "             fmt='o', capsize=3, capthick=1, ecolor='grey')\n",
    "plt.plot(sorted(y_val[:,1]), sorted(y_val[:,1]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,1]), np.max(y_val[:,1]))\n",
    "plt.ylim(0.65, 1)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$S_8$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff7f13-5d73-4df3-badf-cf72bdc946b6",
   "metadata": {
    "id": "19ff7f13-5d73-4df3-badf-cf72bdc946b6"
   },
   "outputs": [],
   "source": [
    "validation_score = Score._score_phase1(\n",
    "    true_cosmo=y_val,\n",
    "    infer_cosmo=mean_val,\n",
    "    errorbar=errorbar_val\n",
    ")\n",
    "print('averaged score:', np.mean(validation_score))\n",
    "print('averaged error bar:', np.mean(errorbar_val, 0))\n",
    "\n",
    "non_zero_mask = y_val != 0\n",
    "percentage_error_masked = np.full_like(y_val, np.nan)  # Initialize with NaN\n",
    "percentage_error_masked[non_zero_mask] = (y_val[non_zero_mask] - mean_val[non_zero_mask]) / y_val[non_zero_mask] * 100\n",
    "\n",
    "# Calculate mean percentage error (NaN values are automatically ignored)\n",
    "mean_percentage_error_masked = np.nanmean(percentage_error_masked, axis=0)\n",
    "\n",
    "print(f\"  Ω_m: {mean_percentage_error_masked[0]:.2f}%\")\n",
    "print(f\"  S_8: {mean_percentage_error_masked[1]:.2f}%\")\n",
    "print(f\"  Overall: {np.nanmean(percentage_error_masked):.2f}%\")\n",
    "\n",
    "\n",
    "# Save the validation score as a JSON file\n",
    "scoring_output = {\"validation_score\": float(validation_score),\n",
    "                  \"num_val\": Nval,\n",
    "                  \"num_train\": Ntrain,\n",
    "                 \"Ω_m%error\": mean_percentage_error_masked[0],\n",
    "                \"S_8%error\":mean_percentage_error_masked[1]}:\n",
    "\n",
    "with open(f\"scoring_output/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(scoring_output, f, indent=2)\n",
    "\n",
    "print(f\"Validation score saved to scoring_output/{MODEL_NAME}.json: {validation_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627d92f-e23a-4161-8d70-6ee9118c3902",
   "metadata": {
    "id": "b627d92f-e23a-4161-8d70-6ee9118c3902"
   },
   "source": [
    "# 6 - Phase one inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e2b7a-aa16-4543-8b1b-032d795d26ad",
   "metadata": {
    "id": "a34e2b7a-aa16-4543-8b1b-032d795d26ad"
   },
   "source": [
    "### Estimate the summary statistics $\\boldsymbol{d}$ for all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2f916-2aba-408e-9cf0-8144eb66decb",
   "metadata": {
    "id": "82d2f916-2aba-408e-9cf0-8144eb66decb"
   },
   "outputs": [],
   "source": [
    "test_dataset = CosmologyDataset(\n",
    "    data=data_obj.kappa_test,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c4191-a14f-4e83-a8ba-1b7c7d641121",
   "metadata": {
    "id": "f48c4191-a14f-4e83-a8ba-1b7c7d641121"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "means_pred_list, sigmas_pred_list = [], []\n",
    "pbar = tqdm(test_loader, total=len(test_loader), desc=\"Inference on the test set\")\n",
    "with torch.no_grad():\n",
    "    for X in pbar:\n",
    "        X = X.to(config.DEVICE)\n",
    "        means_pred, sigmas_pred = model(X)\n",
    "        means_pred_list.append(means_pred.cpu().numpy())\n",
    "        sigmas_pred_list.append(sigmas_pred.cpu().numpy())\n",
    "\n",
    "mean = np.concatenate(means_pred_list, axis=0)\n",
    "mean = label_scaler.inverse_transform(mean)          # inverse transform\n",
    "\n",
    "errorbar = np.concatenate(sigmas_pred_list, axis=0)\n",
    "errorbar = errorbar*label_scaler.var_**0.5           # rescale by the training label std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed0f32-14a9-4ac4-9bd8-686f26fac810",
   "metadata": {
    "id": "4eed0f32-14a9-4ac4-9bd8-686f26fac810"
   },
   "outputs": [],
   "source": [
    "## Include the prior that the cosmological parameters are not negative\n",
    "negative_mask = mean - errorbar < 0\n",
    "errorbar[negative_mask] = mean[negative_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74bd85-f325-4556-87fb-ecf72b1482a0",
   "metadata": {
    "id": "cf74bd85-f325-4556-87fb-ecf72b1482a0"
   },
   "source": [
    "#### ⚠️ NOTE:\n",
    "- `mean`: a 2D array containing the point estimates of 2 cosmological parameters $\\hat{\\Omega}_m$ and $\\hat{S}_8$.\n",
    "- `errorbar`: a 2D array containing the one-standard deviation uncertainties of 2 cosmological parameters $\\hat{\\sigma}_{\\Omega_m}$ and  $\\hat{\\sigma}_{S_8}$.\n",
    "\n",
    "The shapes of `mean`, and `errorbar` must be $(N_{\\rm test}, 2)$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede87bdd",
   "metadata": {
    "id": "ede87bdd"
   },
   "source": [
    "# 7 - (Optional) Prepare submission for Codabench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf7cb8-cf09-4e12-8129-2a0bd06b147c",
   "metadata": {
    "id": "bccf7cb8-cf09-4e12-8129-2a0bd06b147c"
   },
   "source": [
    "***\n",
    "\n",
    "This section will save the model predictions `mean` and `errorbar` (both are 2D arrays with shape `(4000, 2)`, where `4000` is the number of test instances and `2` is the number of our parameters of interest) as a dictionary in a JSON file `result.json`. Then it will compress `result.json` into a zip file that can be directly submitted to Codabench.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2700348",
   "metadata": {
    "id": "d2700348"
   },
   "outputs": [],
   "source": [
    "data = {\"means\": mean.tolist(), \"errorbars\": errorbar.tolist()}\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "zip_file_name = f'Submission_{MODEL_NAME}' + the_date + '.zip'\n",
    "zip_file = Utility.save_json_zip(\n",
    "    submission_dir=\"submissions\",\n",
    "    json_file_name=\"result.json\",\n",
    "    zip_file_name=zip_file_name,\n",
    "    data=data\n",
    ")\n",
    "print(f\"Submission ZIP saved at: {zip_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb316698-c423-4977-971e-e88cb11046ff",
   "metadata": {
    "id": "fb316698-c423-4977-971e-e88cb11046ff"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
