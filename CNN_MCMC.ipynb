{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a25f5e3-cc02-4cc0-8cb1-53c9f066d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069869cd-9501-47a5-8c32-c8f3a7f25756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is /pscratch/sd/l/lindajin/WL_ML\n"
     ]
    }
   ],
   "source": [
    "# USE_PUBLIC_DATASET = False\n",
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)\n",
    "\n",
    "USE_PUBLIC_DATASET = True\n",
    "PUBLIC_DATA_DIR = os.path.join(root_dir, 'input_data/') # This is only required when you set USE_PUBLIC_DATASET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be959a29-b52e-4e8e-b20e-e6f2c507b710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is /pscratch/sd/l/lindajin/WL_ML\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)\n",
    "if not USE_PUBLIC_DATASET:                                         # Testing this startking kit with a tiny sample of the training data (3, 30, 1424, 176)\n",
    "    DATA_DIR = os.path.join(root_dir, 'input_data/')\n",
    "else:                                                              # Training your model with all training data (101, 256, 1424, 176)\n",
    "    DATA_DIR = PUBLIC_DATA_DIR    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51693e-73ba-4f29-adf0-7a52785f210e",
   "metadata": {},
   "source": [
    "### Load the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4be679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data class object\n",
    "data_obj = Data(data_dir=DATA_DIR, USE_PUBLIC_DATASET=USE_PUBLIC_DATASET)\n",
    "\n",
    "# Load test data\n",
    "data_obj.load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b475663b-ca7c-4443-af4a-e1e3eb416a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 101 cosmological models, each has 256 realizations of nuisance parameters in the training data.\n"
     ]
    }
   ],
   "source": [
    "Ncosmo = data_obj.Ncosmo\n",
    "Nsys = data_obj.Nsys\n",
    "\n",
    "print(f'There are {Ncosmo} cosmological models, each has {Nsys} realizations of nuisance parameters in the training data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e88dd-7653-4188-bbab-a23ebd6264e0",
   "metadata": {},
   "source": [
    "#### ⚠️ NOTE:\n",
    "\n",
    "If you want to split your own training/validation sets to evaluate your model, we recommend splitting the original training data along `axis = 1` (the 256 realizations of nuisance parameters). This will ensure that there are no intrinsic correlations between the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff8bb2f-0acf-48bc-90d5-58e0c1e38709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved split data (if you saved it at DATA_DIR before)\n",
    "\n",
    "noisy_kappa_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_train.npy\")\n",
    "label_train = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_train.npy\")\n",
    "noisy_kappa_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"noisy_kappa_val.npy\")\n",
    "label_val = Utility.load_np(data_dir=DATA_DIR, file_name=\"label_val.npy\")\n",
    "\n",
    "Ntrain = label_train.shape[0]*label_train.shape[1]\n",
    "Nval = label_val.shape[0]*label_val.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b77f1c-1c2b-46ef-9247-147f914fc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for CNN\n",
    "X_train = noisy_kappa_train.reshape(Ntrain, *data_obj.shape)\n",
    "X_val = noisy_kappa_val.reshape(Nval, *data_obj.shape)\n",
    "\n",
    "# Here, we ignore the nuisance parameters and only keep the 2 cosmological parameters\n",
    "y_train = label_train.reshape(Ntrain, 5)[:, :2]\n",
    "y_val = label_val.reshape(Nval, 5)[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ad216b-9db5-4ea4-ac26-48004b611e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the split training data = (20604, 1424, 176)\n",
      "Shape of the split validation data = (5252, 1424, 176)\n",
      "Shape of the split training labels = (20604, 2)\n",
      "Shape of the split validation labels = (5252, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of the split training data = {X_train.shape}')\n",
    "print(f'Shape of the split validation data = {X_val.shape}')\n",
    "\n",
    "print(f'Shape of the split training labels = {y_train.shape}')\n",
    "print(f'Shape of the split validation labels = {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efab9b-9e44-4ab2-bc89-ac81adcd99a4",
   "metadata": {},
   "source": [
    "# 4 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa451c18-7541-400a-9d68-9308d99bd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your path for saving the trained model\n",
    "MODEL_NAME = \"ResNetWithAttention_MCMC_MSE\"\n",
    "\n",
    "class Config:\n",
    "    IMG_HEIGHT = data_obj.shape[0]\n",
    "    IMG_WIDTH = data_obj.shape[1]\n",
    "    \n",
    "    # Parameters to predict (Omega_m, S_8)\n",
    "    NUM_TARGETS = 2\n",
    "\n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 2e-4\n",
    "    WEIGHT_DECAY = 1e-4   # L2 regularization to prevent overfitting\n",
    "    \n",
    "    DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    # DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    MODEL_SAVE_PATH = os.path.join(root_dir, f\"{MODEL_NAME}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1a905-64ac-4d9e-ae98-ca37efe709bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN architecture for parameter estimation\n",
    "\n",
    "class Simple_CNN(nn.Module):\n",
    "    def __init__(self, height, width, num_targets):\n",
    "        super(Simple_CNN, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self._feature_size = self._get_conv_output_size(height, width)\n",
    "        \n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self._feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_targets)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output_size(self, height, width):\n",
    "        dummy_input = torch.zeros(1, 1, height, width)\n",
    "        output = self.conv_stack(dummy_input)\n",
    "        return int(np.prod(output.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.fc_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd539c2e-44d8-4e04-b366-eb8f93df596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Training\")\n",
    "    for X, y in pbar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, total=len(dataloader), desc=\"Validating\")\n",
    "    with torch.no_grad():\n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0171198-8a56-47a4-9d1c-bf754257d7b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class CosmologyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, labels=None,\n",
    "                 transform=None,\n",
    "                 label_transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.label_transform = label_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert NumPy array to float64 before transform\n",
    "        image = self.data[idx].astype(np.float64)   \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # Convert PyTorch tensor to double precision\n",
    "            image = image.double()\n",
    "        else:\n",
    "            # If no transform, convert NumPy to PyTorch tensor manually\n",
    "            image = torch.from_numpy(image).double()\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx].astype(np.float64)\n",
    "            label = torch.from_numpy(label).double()  # Ensure double precision\n",
    "            if self.label_transform:\n",
    "                label = self.label_transform(label)\n",
    "                label = label.double()  # Ensure still double after transform\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77aca1f-202f-4213-a3ea-e2636120d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the means and stds of the training images (for standardizing the data)\n",
    "\n",
    "means = np.mean(X_train, dtype=np.float32)\n",
    "stds = np.std(X_train, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80b5a6-9c1b-4be4-abed-0b061055d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image stats (from train set): Mean=-0.00014721968909725547, Std=0.020474432036280632\n",
      "Label stats (from train set): Mean=[0.29021683 0.81345297], Std=[0.1055216  0.06600116]\n"
     ]
    }
   ],
   "source": [
    "# Image standardization\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),     \n",
    "    transforms.Normalize(mean=[means], std=[stds]),   \n",
    "])\n",
    "print(f\"Image stats (from train set): Mean={means}, Std={stds}\")\n",
    "\n",
    "# Label standardization\n",
    "label_scaler = StandardScaler()\n",
    "y_train_scaled = label_scaler.fit_transform(y_train)\n",
    "y_val_scaled = label_scaler.transform(y_val)\n",
    "print(f\"Label stats (from train set): Mean={label_scaler.mean_}, Std={np.sqrt(label_scaler.var_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f83eef-6a50-4560-a5c0-1036bce7a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration\n",
    "config = Config()\n",
    "print(f\"Using device: {config.DEVICE}\")\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = CosmologyDataset(\n",
    "    data=X_train, \n",
    "    labels=y_train_scaled,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset = CosmologyDataset(\n",
    "    data=X_val, \n",
    "    labels=y_val_scaled,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d1060-d2fc-4579-87ea-e81eb21926f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the CNN model\n",
    "# model = Simple_CNN(config.IMG_HEIGHT,\n",
    "#                     config.IMG_WIDTH,\n",
    "#                     config.NUM_TARGETS).to(config.DEVICE)\n",
    "import model\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "from model import BigGANUNet2DModel, ResNetWithAttention\n",
    "\n",
    "\n",
    "model = ResNetWithAttention(config.IMG_HEIGHT,\n",
    "                    config.IMG_WIDTH,\n",
    "                    config.NUM_TARGETS).to(config.DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae4dee-515d-4404-b8af-1601ffa0f79c",
   "metadata": {},
   "source": [
    "Set `USE_PRETRAINED_MODEL = False` if you want to train a new model.\\\n",
    "Set `USE_PRETRAINED_MODEL = True` if you want to load a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e7b94-3579-49ef-b13a-fc60bbb079ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL = False\n",
    "# USE_PRETRAINED_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b93c37-9e07-4260-a1da-938a63ecca18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 103/161 [38:05<15:33, 16.10s/it] "
     ]
    }
   ],
   "source": [
    "if not USE_PRETRAINED_MODEL:  \n",
    "    # Train the model\n",
    "    loss_fn = nn.MSELoss() \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                lr=config.LEARNING_RATE,\n",
    "                                weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = ReduceLROnPlateau(optimizer,\n",
    "                                  mode='min',\n",
    "                                  factor=0.5,\n",
    "                                  patience=5)\n",
    "    # Training Loop\n",
    "    best_val_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, config.DEVICE)\n",
    "        val_loss = validate_epoch(model, val_loader, loss_fn, config.DEVICE)\n",
    "    \n",
    "        scheduler.step(val_loss)    \n",
    "        print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Save the best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), config.MODEL_SAVE_PATH)\n",
    "            print(f\"  -> New best model saved to {config.MODEL_SAVE_PATH}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTraining finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(config.MODEL_SAVE_PATH, weights_only=True)) # Directly load the best model\n",
    "\n",
    "else:\n",
    "    # Check if the pretrained model exists\n",
    "    if os.path.exists(config.MODEL_SAVE_PATH):\n",
    "        # If the pretrained model exists, load the model\n",
    "        model.load_state_dict(torch.load(config.MODEL_SAVE_PATH, weights_only=True))\n",
    "\n",
    "    else:\n",
    "        # If the pretrained model doesn't exist, show the warning message\n",
    "        warning_msg = f\"The path of pretrained model doesn't exist\"\n",
    "        warnings.warn(warning_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b294230d-cb68-41a2-a94a-c65d9d877012",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5 - Inference on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc5683a-0288-4b3e-a708-99179d5775fe",
   "metadata": {},
   "source": [
    "Similar to the power spectrum analysis, we can obtain the point estimates and their uncertainties by the sampled posterior distribution using MCMC. We have to calculate the mean summary statistics and covariance for $N_{\\rm cosmo}$ cosmological models.\n",
    "\n",
    "The mean summary statistics (descriptors) across the $N$ realizations for each of the $N_{\\rm cosmo}$ cosmological models is\n",
    "$$\\mu(\\boldsymbol{\\theta}) \\equiv \\left\\langle\\boldsymbol{d}^{j}\\right\\rangle =\\frac{1}{N} \\sum_{j=1}^{N} \\boldsymbol{d}^{j}~,$$\n",
    "where $\\boldsymbol{d}^{j}$ is the summary statistics of the $j$-th realizations of a given cosmological model.\n",
    "\n",
    "The covariance matrix of the summary statistics, used later for parameter inference, is\n",
    "$$\\operatorname{Cov}(\\boldsymbol{\\theta})=\\frac{1}{N-n_d-2} \\sum_{j=1}^{N}~[\\boldsymbol{d}^{j}-\\mu(\\boldsymbol{\\theta})]^T ~[\\boldsymbol{d}^{j}-\\mu(\\boldsymbol{\\theta})],$$\n",
    "where $n_d$ is the number of summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfc931-d1f3-47a0-b2c4-23f983c1e2bd",
   "metadata": {},
   "source": [
    "The Gaussian likelihood of an observation (represented by summary statistics $\\boldsymbol{d}$ given parameters $\\boldsymbol{\\theta}$ is:\n",
    "$$\n",
    "p(\\boldsymbol{d}|\\boldsymbol{\\theta}) \\propto \\frac{1}{\\sqrt{|{\\rm Cov}(\\boldsymbol{\\theta})|}} \\exp \\left\\{-\\frac{1}{2}[\\boldsymbol{d}-\\mu(\\boldsymbol{\\theta})]^T {\\rm Cov}^{-1}(\\boldsymbol{\\theta})[\\boldsymbol{d}-\\mu(\\boldsymbol{\\theta})]\\right\\}~.\n",
    "$$\n",
    "\n",
    "The log likelihood is thus:\n",
    "$$\n",
    "{\\rm log}~ p(\\boldsymbol{d}|\\boldsymbol{\\theta}) = -\\frac{1}{2}{\\rm log}~ |{\\rm Cov}(\\boldsymbol{\\theta})|  -\\frac{1}{2}[\\boldsymbol{d}-\\mu(\\boldsymbol{\\theta})]^T {\\rm Cov}^{-1}(\\boldsymbol{\\theta})[\\boldsymbol{d}-\\mu(\\boldsymbol{\\theta})] + \\text{(an offset of the normalization)}~.\n",
    "$$\n",
    "\n",
    "In this notebook, we use the outputs of the convolutional neural network (the point estimates of cosmological parameters $\\boldsymbol{\\theta} = (\\hat{\\Omega}_m, \\hat{S}_8)$) as the summary statistic to constrain the cosmological parameters, so\n",
    "$$\n",
    "\\boldsymbol{d} = f_{\\rm NN}^{\\phi} ~~\\text{ with $n_d = 2$.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c840f2-cc96-428f-be9e-9010c0e0ba70",
   "metadata": {},
   "source": [
    "### Calculate the summary statistics $\\boldsymbol{d}$ for all maps in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b9cb2-1d21-40b7-be72-6a3f4bb3e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []   \n",
    "pbar = tqdm(val_loader, total=len(val_loader), desc=\"Validating\")\n",
    "with torch.no_grad():\n",
    "    for X, _ in pbar:\n",
    "        X = X.to(config.DEVICE)\n",
    "        y_pred = model(X)        \n",
    "        y_pred = label_scaler.inverse_transform(y_pred.cpu().numpy())\n",
    "        y_pred_list.append(y_pred) \n",
    "\n",
    "y_pred_val = np.concatenate(y_pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e771d5e-7570-4f34-8304-c22dc042154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the CNN predictions and the validation labels\n",
    "\n",
    "plt.scatter(y_val[:,0], y_pred_val[:,0])\n",
    "plt.plot(sorted(y_val[:,0]), sorted(y_val[:,0]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,0]), np.max(y_val[:,0]))\n",
    "plt.ylim(0, 0.7)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$\\Omega_m$')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_val[:,1], y_pred_val[:,1])\n",
    "plt.plot(sorted(y_val[:,1]), sorted(y_val[:,1]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,1]), np.max(y_val[:,1]))\n",
    "plt.ylim(0.65, 1)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$S_8$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5e5f6-4e69-494a-ac21-3fd583ef8473",
   "metadata": {},
   "source": [
    "### Mean summary statistics and covariance for $N_{\\rm cosmo}$ cosmological model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f399a9-629d-4dd4-b528-1d17517cbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are Ncosmo distinct cosmologies in the labels.\n",
    "# Here we create a list that groups the indices of the validation instances with the same cosmological parameters\n",
    "\n",
    "cosmology = data_obj.label[:,0,:2]   # shape = (Ncosmo, 2)\n",
    "\n",
    "row_to_i = {tuple(cosmology[i]): i for i in range(Ncosmo)}\n",
    "index_lists = [[] for _ in range(cosmology.shape[0])]\n",
    "\n",
    "# Loop over each row in 'y_val' with shape = (Nval, 2)\n",
    "for idx in range(len(y_val)):\n",
    "    row_tuple = tuple(y_val[idx])\n",
    "    i = row_to_i[row_tuple]\n",
    "    index_lists[i].append(idx)\n",
    "\n",
    "# val_cosmology_idx[i] = the indices idx of the validation examples with labels = cosmology[i]\n",
    "val_cosmology_idx = [np.array(lst) for lst in index_lists]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26a7bf-71e8-4e23-82ea-89a82d7f513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summary statistics of all realizations for all cosmologies in the validation set\n",
    "d_vector = []  \n",
    "n_d = 2   # Number of summary statistics for each map\n",
    "for i in range(Ncosmo):\n",
    "    d_i =  np.zeros((len(val_cosmology_idx[i]), n_d))  \n",
    "    for j, idx in enumerate(val_cosmology_idx[i]):\n",
    "        d_i[j] = y_pred_val[idx]\n",
    "\n",
    "    d_vector.append(d_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae67b2-2443-4bc1-b89f-d2821bafc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean summary statistics (average over all realizations)\n",
    "mean_d_vector = []\n",
    "for i in range(Ncosmo):\n",
    "    mean_d_vector.append(np.mean(d_vector[i], 0))\n",
    "mean_d_vector = np.array(mean_d_vector)   \n",
    "\n",
    "# covariance matrix\n",
    "delta = []\n",
    "for i in range(Ncosmo):\n",
    "    delta.append((d_vector[i] - mean_d_vector[i].reshape(1, n_d))) \n",
    "\n",
    "cov_d_vector = [(delta[i].T @ delta[i] / (len(delta[i])-n_d-2))[None] for i in range(Ncosmo)]     \n",
    "cov_d_vector = np.concatenate(cov_d_vector, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccc3d0-2feb-4ccb-afcb-5c4c73afb02e",
   "metadata": {},
   "source": [
    "### Summary statistics emulator (linear interpolation between $N_{\\rm cosmo}$ cosmological models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005939ee-2be6-4c34-a13f-0c90fb2e1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import LinearNDInterpolator\n",
    "mean_d_vector_interp = LinearNDInterpolator(cosmology, mean_d_vector, fill_value=np.nan)\n",
    "cov_d_vector_interp = LinearNDInterpolator(cosmology, cov_d_vector, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b49fe-8aa9-4eb1-b82b-27d90ed0ab75",
   "metadata": {},
   "source": [
    "### Define prior, likelihood, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf041cb-e52c-46a9-a657-b8047363f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior_interp = LinearNDInterpolator(cosmology, np.zeros((Ncosmo, 1)), fill_value=-np.inf)\n",
    "\n",
    "# Note that the training data are not uniformly sampled, which introduces a prior distribution. Here we ignore that prior for simplicity.\n",
    "# Also note that this prior would introduce bias for cosmologies at the boundary of the prior\n",
    "def log_prior(x):\n",
    "    logprior = logprior_interp(x).flatten()  # shape = (Ntest, ) \n",
    "    return logprior\n",
    "\n",
    "# Gaussian likelihood with interpolated mean and covariance matrix\n",
    "def loglike(x, d):\n",
    "    mean = mean_d_vector_interp(x) \n",
    "    cov = cov_d_vector_interp(x)   \n",
    "    delta = d - mean               \n",
    "    \n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    cov_det = np.linalg.slogdet(cov)[1]\n",
    "    \n",
    "    return -0.5 * cov_det - 0.5 * np.einsum(\"ni,nij,nj->n\", delta, inv_cov, delta)\n",
    "\n",
    "def logp_posterior(x, d):\n",
    "    logp = log_prior(x)\n",
    "    select = np.isfinite(logp)\n",
    "    if np.sum(select) > 0:\n",
    "        logp[select] = logp[select] + loglike(x[select], d[select])\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99948402-9e85-459e-b3f7-98e3fbc34d58",
   "metadata": {},
   "source": [
    "### Quickly check the score on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c282a-bbce-4ddd-8ace-4ce7129e3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sampling to explore the posterior distribution\n",
    "\n",
    "Nstep = 10000  # Number of MCMC steps (iterations)\n",
    "sigma = 0.06   # Proposal standard deviation; should be tuned per method or parameter scale\n",
    "\n",
    "# Randomly select initial points from the `cosmology` array for each test case\n",
    "# Assumes `cosmology` has shape (Ncosmo, ndim) and `Ntest` is the number of independent chains/samples\n",
    "current = cosmology[np.random.choice(Ncosmo, size=Nval)]\n",
    "\n",
    "# Compute log-posterior at the initial points\n",
    "curr_logprob = logp_posterior(current, y_pred_val)\n",
    "\n",
    "# List to store sampled states (for all chains)\n",
    "states = []\n",
    "\n",
    "# Track total acceptance probabilities to compute acceptance rates\n",
    "total_acc = np.zeros(len(current))\n",
    "\n",
    "t = time.time()  # Track time for performance reporting\n",
    "\n",
    "# MCMC loop\n",
    "for i in range(Nstep):\n",
    "\n",
    "    # Generate proposals by adding Gaussian noise to current state\n",
    "    proposal = current + np.random.randn(*current.shape) * sigma    \n",
    "\n",
    "    # Compute log-posterior at the proposed points\n",
    "    proposal_logprob = logp_posterior(proposal, y_pred_val)\n",
    "\n",
    "    # Compute log acceptance ratio (Metropolis-Hastings)\n",
    "    acc_logprob = proposal_logprob - curr_logprob\n",
    "    acc_logprob[acc_logprob > 0] = 0  # Cap at 0 to avoid exp overflow (acceptance prob ≤ 1)\n",
    "\n",
    "    # Convert to acceptance probabilities\n",
    "    acc_prob = np.exp(acc_logprob)\n",
    "\n",
    "    # Decide whether to accept each proposal\n",
    "    acc = np.random.uniform(size=len(current)) < acc_prob\n",
    "\n",
    "    # Track acceptance probabilities (not binary outcomes)\n",
    "    total_acc += acc_prob\n",
    "\n",
    "    # Update states and log-probs where proposals are accepted\n",
    "    current[acc] = proposal[acc]\n",
    "    curr_logprob[acc] = proposal_logprob[acc]\n",
    "\n",
    "    # Save a copy of the current state\n",
    "    states.append(np.copy(current)[None])\n",
    "\n",
    "    # Periodically print progress and acceptance rates\n",
    "    if i % (0.1*Nstep) == 0.1*Nstep-1:\n",
    "        print(\n",
    "            'step:', len(states),\n",
    "            'Time:', time.time() - t,\n",
    "            'Min acceptance rate:', np.min(total_acc / (i + 1)),\n",
    "            'Mean acceptance rate:', np.mean(total_acc / (i + 1))\n",
    "        )\n",
    "        t = time.time()  # Reset timer for next print interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d221020-1f20-452c-8f30-8384643a5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove burn-in\n",
    "states = np.concatenate(states[int(0.2*Nstep):], 0)\n",
    "\n",
    "# mean and std of samples\n",
    "mean_val = np.mean(states, 0)\n",
    "errorbar_val = np.std(states, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c417b0-01f4-4dc4-b301-d609edaec917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of the means & standard deviations of the posterior distributions and the validation labels\n",
    "\n",
    "plt.errorbar(y_val[:,0], mean_val[:,0], yerr=errorbar_val[:,0], \n",
    "             fmt='o', capsize=3, capthick=1, ecolor='grey')\n",
    "plt.plot(sorted(y_val[:,0]), sorted(y_val[:,0]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,0]), np.max(y_val[:,0]))\n",
    "plt.ylim(0, 0.7)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$\\Omega_m$')\n",
    "plt.show()\n",
    "\n",
    "plt.errorbar(y_val[:,1], mean_val[:,1], yerr=errorbar_val[:,1], \n",
    "             fmt='o', capsize=3, capthick=1, ecolor='grey')\n",
    "plt.plot(sorted(y_val[:,1]), sorted(y_val[:,1]),\n",
    "         color = 'grey', linestyle='dashed')\n",
    "plt.xlim(np.min(y_val[:,1]), np.max(y_val[:,1]))\n",
    "plt.ylim(0.65, 1)\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(r'$S_8$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff7f13-5d73-4df3-badf-cf72bdc946b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_score = Score._score_phase1(\n",
    "    true_cosmo=y_val,\n",
    "    infer_cosmo=mean_val,\n",
    "    errorbar=errorbar_val\n",
    ")\n",
    "print('averaged score:', np.mean(validation_score))\n",
    "print('averaged error bar:', np.mean(errorbar_val, 0))\n",
    "\n",
    "non_zero_mask = y_val != 0\n",
    "percentage_error_masked = np.full_like(y_val, np.nan)  # Initialize with NaN\n",
    "percentage_error_masked[non_zero_mask] = (y_val[non_zero_mask] - mean_val[non_zero_mask]) / y_val[non_zero_mask] * 100\n",
    "\n",
    "# Calculate mean percentage error (NaN values are automatically ignored)\n",
    "mean_percentage_error_masked = np.nanmean(percentage_error_masked, axis=0)\n",
    "\n",
    "print(f\"  Ω_m: {mean_percentage_error_masked[0]:.2f}%\")\n",
    "print(f\"  S_8: {mean_percentage_error_masked[1]:.2f}%\")\n",
    "print(f\"  Overall: {np.nanmean(percentage_error_masked):.2f}%\")\n",
    "\n",
    "\n",
    "# Save the validation score as a JSON file\n",
    "scoring_output = {\"validation_score\": float(validation_score),\n",
    "                  \"num_val\": Nval,\n",
    "                  \"num_train\": Ntrain,\n",
    "                 \"Ω_m%error\": mean_percentage_error_masked[0],\n",
    "                \"S_8%error\":mean_percentage_error_masked[1]}:\n",
    "\n",
    "with open(f\"scoring_output/{MODEL_NAME}.json\", \"w\") as f:\n",
    "    json.dump(scoring_output, f, indent=2)\n",
    "\n",
    "print(f\"Validation score saved to scoring_output/{MODEL_NAME}.json: {validation_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627d92f-e23a-4161-8d70-6ee9118c3902",
   "metadata": {},
   "source": [
    "# 6 - Phase one inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e2b7a-aa16-4543-8b1b-032d795d26ad",
   "metadata": {},
   "source": [
    "### Estimate the summary statistics $\\boldsymbol{d}$ for all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2f916-2aba-408e-9cf0-8144eb66decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CosmologyDataset(\n",
    "    data=data_obj.kappa_test, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c4191-a14f-4e83-a8ba-1b7c7d641121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []   \n",
    "pbar = tqdm(test_loader, total=len(test_loader), desc=\"Inference on the test set\")\n",
    "with torch.no_grad():\n",
    "    for X in pbar:\n",
    "        X = X.to(config.DEVICE)\n",
    "        y_pred = model(X)        \n",
    "        y_pred = label_scaler.inverse_transform(y_pred.cpu().numpy())\n",
    "        y_pred_list.append(y_pred) \n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5de69b-da82-4f14-9cf1-30dff7713b99",
   "metadata": {},
   "source": [
    "### Sample the posterior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edafeab-6d65-4fc6-9b2f-d596774f2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sampling to explore the posterior distribution\n",
    "\n",
    "Nstep = 10000  # Number of MCMC steps (iterations)\n",
    "sigma = 0.06   # Proposal standard deviation; should be tuned per method or parameter scale\n",
    "\n",
    "# Randomly select initial points from the `cosmology` array for each test case\n",
    "# Assumes `cosmology` has shape (Ncosmo, ndim) and `Ntest` is the number of independent chains/samples\n",
    "current = cosmology[np.random.choice(Ncosmo, size=data_obj.Ntest)]\n",
    "\n",
    "# Compute log-posterior at the initial points\n",
    "curr_logprob = logp_posterior(current, y_pred_test)\n",
    "\n",
    "# List to store sampled states (for all chains)\n",
    "states = []\n",
    "\n",
    "# Track total acceptance probabilities to compute acceptance rates\n",
    "total_acc = np.zeros(len(current))\n",
    "\n",
    "t = time.time()  # Track time for performance reporting\n",
    "\n",
    "# MCMC loop\n",
    "for i in range(Nstep):\n",
    "\n",
    "    # Generate proposals by adding Gaussian noise to current state\n",
    "    proposal = current + np.random.randn(*current.shape) * sigma    \n",
    "\n",
    "    # Compute log-posterior at the proposed points\n",
    "    proposal_logprob = logp_posterior(proposal, y_pred_test)\n",
    "\n",
    "    # Compute log acceptance ratio (Metropolis-Hastings)\n",
    "    acc_logprob = proposal_logprob - curr_logprob\n",
    "    acc_logprob[acc_logprob > 0] = 0  # Cap at 0 to avoid exp overflow (acceptance prob ≤ 1)\n",
    "\n",
    "    # Convert to acceptance probabilities\n",
    "    acc_prob = np.exp(acc_logprob)\n",
    "\n",
    "    # Decide whether to accept each proposal\n",
    "    acc = np.random.uniform(size=len(current)) < acc_prob\n",
    "\n",
    "    # Track acceptance probabilities (not binary outcomes)\n",
    "    total_acc += acc_prob\n",
    "\n",
    "    # Update states and log-probs where proposals are accepted\n",
    "    current[acc] = proposal[acc]\n",
    "    curr_logprob[acc] = proposal_logprob[acc]\n",
    "\n",
    "    # Save a copy of the current state\n",
    "    states.append(np.copy(current)[None])\n",
    "\n",
    "    # Periodically print progress and acceptance rates\n",
    "    if i % (0.1*Nstep) == 0.1*Nstep-1:\n",
    "        print(\n",
    "            'step:', len(states),\n",
    "            'Time:', time.time() - t,\n",
    "            'Min acceptance rate:', np.min(total_acc / (i + 1)),\n",
    "            'Mean acceptance rate:', np.mean(total_acc / (i + 1))\n",
    "        )\n",
    "        t = time.time()  # Reset timer for next print interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e4ca8-a9f6-4753-9ca0-6d7c3e8eedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove burn-in\n",
    "states = np.concatenate(states[int(0.2*Nstep):], 0)\n",
    "\n",
    "# mean and std of samples\n",
    "mean = np.mean(states, 0)\n",
    "errorbar = np.std(states, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74bd85-f325-4556-87fb-ecf72b1482a0",
   "metadata": {},
   "source": [
    "#### ⚠️ NOTE:\n",
    "- `mean`: a 2D array containing the point estimates of 2 cosmological parameters $\\hat{\\Omega}_m$ and $\\hat{S}_8$.\n",
    "- `errorbar`: a 2D array containing the one-standard deviation uncertainties of 2 cosmological parameters $\\hat{\\sigma}_{\\Omega_m}$ and  $\\hat{\\sigma}_{S_8}$.\n",
    " \n",
    "The shapes of `mean`, and `errorbar` must be $(N_{\\rm test}, 2)$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede87bdd",
   "metadata": {},
   "source": [
    "# 7 - (Optional) Prepare submission for Codabench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf7cb8-cf09-4e12-8129-2a0bd06b147c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "This section will save the model predictions `mean` and `errorbar` (both are 2D arrays with shape `(4000, 2)`, where `4000` is the number of test instances and `2` is the number of our parameters of interest) as a dictionary in a JSON file `result.json`. Then it will compress `result.json` into a zip file that can be directly submitted to Codabench.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2700348",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"means\": mean.tolist(), \"errorbars\": errorbar.tolist()}\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "zip_file_name = f'Submission_{MODEL_NAME}' + the_date + '.zip'\n",
    "zip_file = Utility.save_json_zip(\n",
    "    submission_dir=\"submissions\",\n",
    "    json_file_name=\"result.json\",\n",
    "    zip_file_name=zip_file_name,\n",
    "    data=data\n",
    ")\n",
    "print(f\"Submission ZIP saved at: {zip_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb316698-c423-4977-971e-e88cb11046ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
